{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c411ddba-9fa7-43e5-8134-d52c94632d66",
   "metadata": {},
   "source": [
    "***\n",
    "# <center>***Introduction to the World of Big Data***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5365cda-414f-4f95-8cba-da523dc5fd42",
   "metadata": {},
   "source": [
    "With the rapid growth of Internet users, there is an exponential growth in the data being generated. The data is generated from millions of messages we send and communicate via WhatsApp, Facebook, or Twitter, from the trillions of photos taken, and hours and hours of videos getting uploaded in YouTube every single minute. According to a recent survey 2.5 quintillion (2 500 000 000 000 000 000, or 2.5 × 1018) bytes of data are generated every day. This enormous amount of data generated is referred to as “big data.” Bigdata does not only mean that the data sets are too large, it is a blanket term for the data that are too large in size, complex in nature, which may be structured or unstructured, and arriving at high velocity as well. Of the data available today, 80 percent has been generated in the last few years. The growth of big data is fueled by the fact that more data are generated on every corner of the world that needs to be captured.\n",
    "\n",
    "Capturing this massive data gives only meager value unless this IT value is transformed into business value. Managing the data and analyzing them have always been beneficial to the organizations; on the other hand, converting these data into valuable business insights has always been the greatest challenge. Data scientists were struggling to find pragmatic techniques to analyze the captured data. The data has to be managed at appropriate speed and time to derive valuable insight from it. These data are so complex that it became difficult to process it using traditional database management systems, which triggered the evolution of the big data era. Additionally, there were constraints on the amount of data that traditional databases could handle. With the increase in the size of \n",
    "data either there was a decrease in performance and increase in latency or it was expensive to add additional memory units. All these limitations have been overcome with the evolution of **big data technologies** that lets us capture, store, process, and analyze the data in a distributed environment. **Examples of Big data technologies are Hadoop, a framework for all big data process, Hadoop Distributed File System (HDFS) for distributed cluster storage, and MapReduce for processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fb4f8-aee1-438b-af18-e65859162918",
   "metadata": {},
   "source": [
    "***\n",
    "## **3 Vs of Big Data:**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaaa62b-73f3-4281-bc6f-2f998f0f0a92",
   "metadata": {},
   "source": [
    "### **1. Volume**\n",
    "Data generated and processed by big data are continuously growing at an ever increasing pace. Volume grows exponentially owing to the fact that business enterprises are continuously capturing the data to make better and bigger business solutions. Big data volume measures from terabytes to zettabytes (1024 GB = 1 terabyte; 1024 TB = 1 petabyte; 1024 PB = 1 exabyte; 1024 EB = 1 zettabyte; 1024 ZB = 1 yottabyte). Capturing this massive data is cited as an extraordinary opportunity to achieve finer customer service and better business advantage. This ever increasing data volume demands highly scalable and reliable storage. The major sources contributing to this tremendous growth in the volume are social media, point of sale (POS) transactions, online banking, GPS sensors, and sensors in vehicles. Facebook generates approximately 500 terabytes of data per day. Every time a link on a website is clicked, an item is purchased online, a video is uploaded in YouTube, data are generated\n",
    "\n",
    "### **2. Velocity**\n",
    "With the dramatic increase in the volume of data, the speed at which the data is generated also surged up. The term “velocity” not only refers to the speed at which data are generated, it also refers to the rate at which data is processed and analyzed. In the big data era, a massive amount of data is generated at high velocity, and sometimes these data arrive so fast that it becomes difficult to capture them, and yet the data needs to be analyzed. \n",
    "\n",
    "### **3. Variety**\n",
    "Variety refers to the format of data supported by big data. Data arrives in structured, semi-structured, and unstructured format. Structured data refers to the data processed by traditional database management systems where the data are organized in tables, such as employee details, bank customer details. Semistructured data is a combination of structured and unstructured data, such as XML. XML data is semi-structured since it does not fit the formal data model (table) associated with traditional database; rather, it contains tags to organize fields within the data. Unstructured data refers to data with no definite structure, such as e-mail messages, photos, and web pages. The data that arrive from Facebook, Twitter feeds, sensors of vehicles, and black boxes of airplanes are all unstructured, which the traditional database cannot process, and here is when big data comes into the picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdb083-24db-4d5d-a630-07c7cebe0b1a",
   "metadata": {},
   "source": [
    "***\n",
    "##  **Different Types of Data**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4c244-8a0e-449c-8943-2bfd58a55885",
   "metadata": {},
   "source": [
    "Data may be machine generated or human generated. Human-generated data refers to the data generated as an outcome of interactions of humans with the machines. E-mails, documents, Facebook posts are some of the human-generated data. Machine-generated data refers to the data generated by computer applications or hardware devices without active human intervention. Data from sensors, disaster warning systems, weather forecasting systems, and satellite data are some of the machine-generated data. Figure 1.6 represents the data generated by a human in various social media, e-mails sent, and pictures that were taken by them and machine data generated by the satellite. \n",
    "\n",
    "*The **machine-generated** and **human-generated** data can be represented by the following primitive types of big data:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca71c3-507d-4c91-b9c8-ec230cf60b5d",
   "metadata": {},
   "source": [
    "### **1. Structured Data**\n",
    "Data that can be stored in a relational database in table format with rows and columns is called structured data. Structured data often generated by business enterprises exhibits a high degree of organization and can easily be processed using data mining tools and can be queried and retrieved using the primary key field. Examples of structured data include employee details and financial transactions. \n",
    "\n",
    "### **2. Unstructured Data**\n",
    "Data that are raw, unorganized, and do not fit into the relational database systems are called unstructured data. Nearly 80% of the data generated are unstructured. Examples of unstructured data include video, audio, images, e-mails, text files, and social media posts. Unstructured data usually reside on either text files or binary files. Data that reside in binary files do not have any identifiable internal structure, for example, audio, video, and images. Data that reside in text files are e-mails, social media posts, pdf files, and word processing documents.\n",
    "\n",
    "### **3. Semi-Structured Data**\n",
    "Semi-structured data are those that have a structure but do not fit into the relational database. Semi-structured data are organized, which makes it easier to analyze when compared to unstructured data. JSON and XML are examples of semi-structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028367be-598c-405d-a904-769bad2f888d",
   "metadata": {},
   "source": [
    "***\n",
    "## **Big Data Infrastructure**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a7b2b-a322-4b52-b7f4-b6748f7512f8",
   "metadata": {},
   "source": [
    "The core components of big data technologies are the tools and technologies that provide the capacity to store, process, and analyze the data. The method of storing the data in tables was no longer supportive with the evolution of data with 3 Vs, namely volume, velocity, and variety. The robust RBDMS was no longer cost effective. The scaling of RDBMS to store and process huge amount of data became expensive. This led to the emergence of new technology, which was highly scalable at very low cost.\n",
    "\n",
    "***The key technologies include:***\n",
    "- Hadoop\n",
    "- HDFS\n",
    "- MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f81c34-09d1-4a00-8a25-faaf0016488a",
   "metadata": {},
   "source": [
    "**1. Hadoop** – Apache Hadoop, written in Java, is open-source framework that supports processing of large data sets. It can store a large volume of structured, semi-structured, and unstructured data in a distributed file system and process them in parallel. It is a highly scalable and cost-effective storage platform. Scalability of Hadoop refers to its capability to sustain its performance even under highly increasing loads by adding more nodes. Hadoop files are written once and read many times. The contents of the files cannot be changed. A large number of computers interconnected working together as a single system is called a cluster. Hadoop clusters are designed to store and analyze the massive amount of disparate data in distributed computing environments in a cost \n",
    "effective manner.\n",
    "\n",
    "**2. Hadoop Distributed File system** – HDFS is designed to store large data sets with streaming access pattern running on low-cost commodity hardware. It does not require highly reliable, expensive hardware. The data set is generated from multiple sources, stored in an HDFS file system in a write-once, read-many-times pattern, and analyses are performed on the data set to extract knowledge from it.\n",
    "\n",
    "**3. MapReduce** – MapReduce is the batch-processing programming model for the Hadoop framework, which adopts a divide-and-conquer principle. It is highly scalable, reliable, and fault tolerant, capable of processing input data with any format in parallel and distributed computing environments supporting only batch workloads. Its performance reduces the processing time significantly compared to the traditional batch-processing paradigm, as the traditional approach was to move the data from the storage platform to the processing platform, whereas the MapReduce processing paradigm resides in the framework where the data actually resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73fc887-6a3d-4ea7-97eb-8b540ced46c1",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Big Data Life Cycle***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a95fad-4f3c-4538-b58c-73300314f8d8",
   "metadata": {},
   "source": [
    "Big data yields big benefits, starting from innovative business ideas to unconventional ways to treat diseases, overcoming the challenges. The challenges arise because so much of the data is collected by the technology today. Big data technologies are capable of capturing and analyzing them effectively. Big data infrastructure involves new computing models with the capability to process both distributed and parallel computations with highly scalable storage and performance. Some of the big data components include Hadoop (framework), HDFS (storage), and MapReduce (processing).\n",
    "\n",
    "Data arriving at high velocity from multiple sources with different data formats are captured. The captured data is stored in a storage platform such as HDFS and NoSQL and then preprocessed to make the data suitable for analysis. The preprocessed data stored in the storage platform is then passed to the analytics layer, where the data is processed using big data tools such as MapReduce and YARN and analysis is performed on the processed data to uncover hidden knowledge from it. Analytics and machine learning are important concepts in the life cycle of big data. Text analytics is a type of analysis performed on unstructured textual data. With the growth of social media and e-mail transactions, the importance of text analytics has surged up. Predictive analysis on consumer behavior and consumer interest analysis are all performed on the text data extracted from various online sources such as social media, online retailing websites, and much more. Machine learning has made text analytics possible. The analyzed data is visually represented by visualization tools such as Tableau to make it easily understandable by the end user to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb95d7-08a8-4690-b4b0-e8db5559c940",
   "metadata": {},
   "source": [
    "***1. Big Data Generation*** - The first phase of the life cycle of big data is the data generation. The scale of data generated from diversified sources is gradually expanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a0b84-49a7-4436-a39b-28b2da8ab25f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "***2 Data Aggregation*** - The data aggregation phase of the big data life cycle involves collecting the raw data, transmitting the data to the storage platform, and preprocessing them. Data acquisition in the big data world means acquiring the high-volume data arriving at an ever-increasing pace. The raw data thus collected is transmitted to a proper storage infrastructure to support processing and various analytical applications. Preprocessing involves data cleansing, data integration, data transformation, and data reduction to make the data reliable, error free, consistent, and accurate. The data gathered may have redundancies, which occupy the storage space and increase the storage cost and can be handled by data preprocessing. Also, much of the data gathered may not be related to the analysis objective, and hence it needs to be compressed while being preprocessed. Hence, efficient data preprocessing is indispensable for cost-effective and efficient data storage. The preprocessed data are then transmitted for various purposes such as data modeling and data analytics.\n",
    "\n",
    "***3 Data Preprocessing*** - Data preprocessing is an important process performed on raw data to transform it into an understandable format and provide access to consistent and accurate data. The data generated from multiple sources are erroneous, incomplete, and inconsistent because of their massive volume and heterogeneous sources, and it is meaningless to store useless and dirty data. Additionally, some analytical applications have a crucial requirement for quality data. Hence, for effective, efficient, and accurate \n",
    "data analysis, systematic data preprocessing is essential. The quality of the source data is affected by various factors. For instance, the data may have errors such as a salary field having a negative value (e.g., salary = −2000), which arises because of \n",
    "transmission errors or typos or intentional wrong data entry by users who do not wish to disclose their personal information. Incompleteness implies that the field lacks the attributes of interest (e.g., Education = “”), which may come from a not applicable field or software errors. Inconsistency in the data refers to the discrepancies in the data, say date of birth and age may be inconsistent. Inconsistencies in data arise when the data collected are from different sources, because of inconsistencies in naming conventions between different countries and inconsistencies in the input format (e.g., date field DD/MM when interpreted as MM/DD). Data sources often have redundant data in different forms, and hence duplicates in the data also have to be removed in data preprocessing to make the data meaningful and error free. There are several steps involved in data preprocessing:\n",
    "  - **Data Integration** - Data integration involves combining data from different sources to give the end users a unified data view. Several challenges are faced while integrating data; as an example, while extracting data from the profile of a person, the first name and family name may be interchanged in a certain culture, so in such cases integration may happen incorrectly. Data redundancies often occur while integrating data from multiple sources.\n",
    "  - **Data Cleaning** -  The data-cleaning process fills in the missing values, corrects the errors and inconsistencies, and removes redundancy in the data to improve the data quality. The larger the heterogeneity of the data sources, the higher the degree of dirtiness. Consequently, more cleaning steps may be involved.\n",
    "  - **Data reduction** - Data reduction is the concept of reducing the volume of data or reducing the dimension of the data, that is, the number of attributes. Data reduction techniques are adopted to analyze the data in reduced format without losing the integrity of the actual data and yet yield quality outputs. Data reduction techniques include data compression, dimensionality reduction, and numerosity reduction.\n",
    "  - **Data Transformation** - Data transformation refers to transforming or consolidating the data into an appropriate format and converting them into logical and meaningful information for data management and analysis. The real challenge in data transformation comes into the picture when fields in one system do not match the fields in another system. Before data ransformation, data cleaning and manipulation takes place. Organizations are collecting a massive amount of data, and the  volume of the data is increasing rapidly. The data captured are transformed using ETL tools. ***Data transformation involves the following strategies:***\n",
    "     - `Smoothing`, which removes noise from the data by incorporating binning, clustering, and regression techniques.\n",
    "     - `Aggregation`, which applies summary or aggregation on the data to give a consolidated data. (E.g., daily profit of an organization may be aggregated to give consolidated monthly or yearly turnover.)\n",
    "     - `Generalization`, which is normally viewed as climbing up the hierarchy where the attributes are generalized to a higher level overlooking the attributes at a lower level. (E.g., street name may be generalized as city name or a higher level hierarchy, namely the country name).\n",
    "     - `Discretization`, which is a technique where raw values in the data (e.g., age) are replaced by conceptual labels (e.g., teen, adult, senior) or interval labels (e.g., 0–9, 10–19, etc.)\n",
    "   \n",
    "***4 Big Data Analytics*** - Businesses are recognizing the unrevealed potential value of this massive data and putting forward the tools and technologies to capitalize on the opportunity. The key to deriving business value from big data is the potential use of analytics. Collecting, storing, and preprocessing the data creates a little value. It has to be analyzed and the end users must make decisions out of the results to derive business value from the data. Big data analytics is a fusion of big data technologies and analytic tools. Analytics is not a new concept: many analytic techniques, namely, regression analysis and machine learning, have existed for many years. Intertwining big data technologies with data from new sources and data analytic techniques is a newly evolved concept. The different types of analytics are descriptive analytics, predictive analytics, and prescriptive analytics.\n",
    "\n",
    "***5 Visualizing Big Data*** -  Visualization makes the life cycle of big data complete assisting the end users to gain insights from the data. From executives to call center employees, everyone wants to extract knowledge from the data collected to assist them in making better decisions. Regardless of the volume of data, one of the best methods to discern relationships and make crucial decisions is to adopt advanced data analysis and visualization tools. Line graphs, bar charts, scatterplots, bubble plots, and pie charts are conventional data visualization techniques. Line graphs are used to depict the relationship between one variable and another. Bar charts are used to compare the values of data belonging to different categories represented by horizontal or vertical bars, whose heights represent the actual values. Scatterplots are used to show the relationship between two variables (X and Y). A bubble plot is a variation of a scatterplot where the relationships between X and Y are displayed in addition to the data value associated with the size of the bubble. Pie charts are used where parts of a whole phenomenon are to be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f09bd-51ec-4bc4-88d6-0d86ef7231c1",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Big Data Technology***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98d1d1-72ef-4af5-8797-e8712ee8bc3f",
   "metadata": {},
   "source": [
    "With the advancement in technology, the ways the data are generated, captured, processed, and analyzed are changing. The efficiency in processing and analyzing the data has improved with the advancement in technology. Thus, technology plays a great role in the entire process of gathering the data to analyzing them and extracting the key insights from the data.\n",
    "\n",
    "**Apache Hadoop** is an open-source platform that is one of the most important technologies of big data. Hadoop is a framework for storing and processing the data. Hadoop was originally created by Doug Cutting and Mike Cafarella, a graduate student from the University of Washington. They jointly worked with the goal of indexing the entire web, and the project is called “Nutch.” The concept of **MapReduce** and **GFS** were integrated into Nutch, which led to the evolution of **Hadoop**. The word **Hadoop** is the \n",
    "name of the toy elephant of Doug’s son. The core components of Hadoop are **HDFS**, Hadoop common, which is a collection of common utilities that support other Hadoop modules, and MapReduce. **Apache Hadoop** is an open-source framework for distributed storage and \n",
    "for processing large data sets. Hadoop can store petabytes of structured, semi-structured, or unstructured data at low cost. The low cost is due to the cluster of commodity hardware on which Hadoop runs.\n",
    "\n",
    "**YARN** – YARN is the acronym for ***Yet Another Resource Negotiator*** and is an open-source framework for distributed  processing. It is the key feature of Hadoop version 2.0 of the Apache software foundation. In Hadoop 1.0 MapReduce was the only component to process the data in distributed environments. Limitations of classical MapReduce have led to the evolution of YARN. The cluster resource management of MapReduce in Hadoop 1.0 was taken over by YARN in Hadoop 2.0. This has lightened up the task of MapReduce and enables it to focus on the data processing part. YARN enables Hadoop to run jobs other than MapReduce jobs as well. Hadoop common – Hadoop common is a collection of common utilities, which supports other Hadoop modules. It is considered as the core module of Hadoop as it offers essential services. Hadoop common has the **scripts** and **Java Archive (JAR)** files that are required to start Hadoop.\n",
    "\n",
    " - **Challenges Faced by Big Data Technology** - Indeed, we are facing a lot of challenges when it comes to dealing with the data. Some data are structured that could be stored in traditional databases, while some are videos, pictures, and documents, which may be unstructured or semi-structured, generated by sensors, social media, satellite, business transactions, and much more. Though these data can be managed independently, the real challenge is how to make sense by integrating disparate data from diversified sources.\n",
    "   - Heterogeneity and incompleteness \n",
    "   - Volume and velocity of the data \n",
    "   - Data storage\n",
    "   - Data privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d2216-759e-485f-b9ac-e0e3af4f7db9",
   "metadata": {},
   "source": [
    "***\n",
    "## ***Big Data Applications***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80482f8-de91-4aa1-ae1a-83a870053095",
   "metadata": {},
   "source": [
    "\n",
    "- `Banking and Securities` – Credit/debit card fraud detection, warning for securities fraud, credit risk reporting, customer data analytics.\n",
    "- `Healthcare sector` – Storing the patient data and analyzing the data to detect various medical ailments at an early stage.\n",
    "- `Marketing` – Analyzing customer purchase history to reach the right customers in order market their newly launched products.\n",
    "- `Web analysis` – Social media data, data from search engines, and so forth, are analyzed to broadcast advertisements based on their interests.\n",
    "- `Call center analytics` – Big data technology is used to identify the recurring problems and staff behavior patterns by capturing and processing the call content.\n",
    "- `Agriculture` – Sensors are used by biotechnology firms to optimize crop efficiency. Big data technology is used in analyzing the sensor data.\n",
    "- `Smartphones` — Facial recognition feature of smart phones is used to unlock their phones, retrieve information about a person with the information previously stored in their smartphones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e00fac-973e-4d8d-a62a-d6aa3fe701b7",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcd49d-de52-4677-b323-ac180ecd2cda",
   "metadata": {},
   "source": [
    "### ***Conceptual Short Questions with Answers***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7967f2-b0c6-49c6-bdc8-b964a9df3fe1",
   "metadata": {},
   "source": [
    "***1 What is big data?***\n",
    "- Big data is a blanket term for the data that are too large in size, complex in nature, which may be structured or unstructured, and arriving at high velocity as well.\n",
    "\n",
    "***2 What are the drawbacks of traditional database that led to the evolution of big data?***\n",
    "- Below are the limitations of traditional databases, which has led to the emergence of big data.\n",
    "   - Exponential increase in data volume, which scales in terabytes and petabytes, has turned out to become a challenge to the RDBMS in handling such a massive volume of data.\n",
    "   - To address this issue, the RDBMS increased the number of processors and added more memory units, which in turn increased the cost.\n",
    "   - Almost 80% of the data fetched were of semi-structured and unstructured format, which RDBMS could not deal with.\n",
    "   - RDBMS could not capture the data coming in at high velocity.\n",
    "\n",
    "***3 What are the factors that explain the tremendous increase in the data volume?***\n",
    "- Multiple disparate data sources are responsible for the tremendous increase in the volume of big data. Much of the growth in data can be attributed to the digitization of almost anything and everything in the globe. Paying e-bills, online shopping, communication through social media, e-mail transactions in various organizations, a digital representation of the organizational data, and so forth, are some of the examples of this digitization around the globe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e699c-527e-4738-8bd8-fb2854e83513",
   "metadata": {},
   "source": [
    "***4 What are the different data types of big data?***\n",
    "- Machine-generated and human-generated data can be represented by the follow\n",
    "ing primitive types of big data\n",
    "    - Structured data\n",
    "    - Unstructured data\n",
    "    - Semi-Structured data\n",
    "\n",
    "***5 What is semi-structured data?***\n",
    "- Semi-structured data are that which have a structure but does not fit into the relational database. Semi-structured data are organized, which makes it easier for analysis when compared to unstructured data. JSON and XML are examples of semi-structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ec257-9e1d-48ff-9a07-e97afa9621cf",
   "metadata": {},
   "source": [
    "***6 What does the three Vs of big data mean?***\n",
    "- Volume–Size of the data\n",
    "     1) Velocity–Rate at which the data is generated and is being processed\n",
    "     2) Variety–Heterogeneity of data: structured, unstructured, and semi-structured\n",
    " \n",
    "**7 What is commodity hardware?**\n",
    "- Commodity hardware is a low-cost, low-performance, and low-specification functional hardware with no distinctive features. Hadoop can run on commodity hardware and does not require any high-end hardware or supercomputers to execute its jobs.\n",
    "\n",
    "***8 What is data aggregation?***\n",
    "- The data aggregation phase of the big data life cycle involves collecting the raw data, transmitting the data to a storage platform, and preprocessing them. Data acquisition in the big data world means acquiring the high-volume data arriving at an ever increasing pace.\n",
    "    \n",
    "***9 What is data preprocessing?***\n",
    "- Data preprocessing is an important process performed on raw data to transform it into an understandable format and provide access to a consistent and an accurate data. The data generated from multiple sources are erroneous, incomplete, and inconsistent because of their massive volume and heterogeneous sources, and it is pointless to store useless and dirty data. Additionally, some analytical applications have a crucial requirement for quality data. Hence, for effective, efficient, and accurate data analysis, systematic data preprocessing is essential.\n",
    "    \n",
    "***10 What is data integration?***\n",
    "- Data integration involves combining data from different sources to give the end users a unified data view.\n",
    "    \n",
    "***11 What is data cleaning?***\n",
    "- The data-cleaning process fills in the missing values, corrects the errors and inconsistencies, and removes redundancy in the data to improve the data quality. The larger the heterogeneity of the data sources, the higher the degree of dirtiness. Consequently, more cleaning steps may be involved.\n",
    "    \n",
    "***12 What is data reduction?***\n",
    "- Data processing on massive data volume may take a long time, making data analysis either infeasible or impractical. Data reduction is the concept of reducing the volume of data or reducing the dimension of the data, that is, the number of attributes. Data reduction techniques are adopted to analyze the data in reduced format without losing the integrity of the actual data and yet yield quality outputs.\n",
    "\n",
    "***13 What is data transformation?***\n",
    "- Data transformation refers to transforming or consolidating the data into an appropriate format that is acceptable by the big data database and converting them into logical and meaningful information for data management and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2f4aa-aecd-4f0d-a9fb-5cf7d45078ec",
   "metadata": {},
   "source": [
    "***14 Give some examples of big data.***\n",
    "- Facebook is generating approximately 500 terabytes of data per day, about 10 terabytes of sensor data are generated every 30 minutes by airlines, the New York Stock Exchange is generating approximately 1 terabyte of data per day. These are examples of big data.\n",
    "\n",
    "***15 How is big data analysis useful for organizations?***\n",
    "- Big data analytics is useful for the organizations to make better decisions, find new business opportunities, compete against business rivals, improve performance and efficiency, and reduce cost by using advanced data analytics techniques.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
